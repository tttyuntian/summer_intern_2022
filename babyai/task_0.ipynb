{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7d63ad",
   "metadata": {},
   "source": [
    "# 1. Generate Task 0 (GoToLocal \"Missing Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31cae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blosc\n",
    "from collections import defaultdict\n",
    "import io\n",
    "import os\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e9090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretraining_data(\n",
    "    env,\n",
    "    data,\n",
    "    output_dir_path,\n",
    "    is_train,\n",
    "    split_size=100,\n",
    "    split_start_id=0,\n",
    "):\n",
    "    def image_to_bytes(image):\n",
    "        bytes_io = io.BytesIO()\n",
    "        image.save(bytes_io, \"PNG\")\n",
    "        return bytes_io.getvalue()\n",
    "    \n",
    "    output_dir = os.path.join(output_dir_path, env)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    split_id = 0\n",
    "    data_split = []\n",
    "    for ex_id, ex in tqdm(enumerate(data), total=len(data)):\n",
    "        img = blosc.unpack_array(ex[2])\n",
    "        img = Image.fromarray(img[0])  # Always select the first frame\n",
    "        img_encoded = image_to_bytes(img)\n",
    "        #data_split[\"img\"].append(img[0])  # Always select the first frame\n",
    "        \n",
    "        #data_split[\"mission\"].append(ex[0])\n",
    "\n",
    "        action = \",\".join(ex[6])\n",
    "        #data_split[\"action_seq\"].append(action)\n",
    "        data_split.append({\n",
    "            \"img\": img_encoded,\n",
    "            \"mission\": ex[0],\n",
    "            \"label\": ex[-1]\n",
    "        })\n",
    "        \n",
    "        if (ex_id+1) % split_size == 0:\n",
    "            if is_train:\n",
    "                output_name = f\"split_train_{split_start_id + split_id}\"\n",
    "            else:\n",
    "                output_name = f\"split_valid_{split_start_id + split_id}\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"{output_name}.pkl\")\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                pkl.dump(data_split, f)\n",
    "            data_split = []\n",
    "            split_id += 1\n",
    "\n",
    "    if data_split:\n",
    "        if is_train:\n",
    "            output_name = f\"split_train_{split_start_id + split_id}\"\n",
    "        else:\n",
    "            output_name = f\"split_valid_{split_start_id + split_id}\"\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{output_name}.pkl\")\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pkl.dump(data_split, f)\n",
    "    \n",
    "    return split_start_id + split_id  # used to track next starting splid_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1020ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positives and negatives\n",
    "with open(f\"../../data/task_0/GoToLocal/positive.pkl\", \"rb\") as f:\n",
    "    positives = pkl.load(f)\n",
    "    for i in range(len(positives)):\n",
    "        positives[i] = positives[i] + (\"Yes\",)\n",
    "    \n",
    "with open(f\"../../data/task_0/GoToLocal/negative.pkl\", \"rb\") as f:\n",
    "    negatives = pkl.load(f)\n",
    "    for i in range(len(negatives)):\n",
    "        negatives[i] = negatives[i] + (\"No\",)\n",
    "    \n",
    "data = positives + negatives\n",
    "\n",
    "with open(f\"../../data/task_0/GoToLocal/Task0_GoToLocal_train_0.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51126367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positives and negatives\n",
    "with open(f\"../../data/task_0/GoToLocal/positive_valid.pkl\", \"rb\") as f:\n",
    "    positives = pkl.load(f)\n",
    "    for i in range(len(positives)):\n",
    "        positives[i] = positives[i] + (\"Yes\",)\n",
    "    \n",
    "with open(f\"../../data/task_0/GoToLocal/negative_valid.pkl\", \"rb\") as f:\n",
    "    negatives = pkl.load(f)\n",
    "    for i in range(len(negatives)):\n",
    "        negatives[i] = negatives[i] + (\"No\",)\n",
    "    \n",
    "data = positives + negatives\n",
    "\n",
    "with open(f\"../../data/task_0/GoToLocal/Task0_GoToLocal_valid_0.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ebc715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 50000/50000 [03:26<00:00, 242.50it/s]\n"
     ]
    }
   ],
   "source": [
    "level_name = \"GoToLocal\"\n",
    "\n",
    "split_start_id = 0\n",
    "\n",
    "for split_id in range(1):\n",
    "    print(split_id)\n",
    "    with open(f\"../../data/task_0/GoToLocal/Task0_{level_name}_train_{split_id}.pkl\", \"rb\") as f:\n",
    "        demos_train = pkl.load(f)\n",
    "    \n",
    "    split_start_id = get_pretraining_data(\n",
    "        level_name, \n",
    "        demos_train, \n",
    "        output_dir_path=\"../../data/task_0\",\n",
    "        is_train=True,\n",
    "        split_size=10000,\n",
    "        split_start_id=split_start_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3131da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:44<00:00, 224.71it/s]\n"
     ]
    }
   ],
   "source": [
    "level_name = \"GoToLocal\"\n",
    "\n",
    "split_start_id = 0\n",
    "\n",
    "for split_id in range(1):\n",
    "    print(split_id)\n",
    "    with open(f\"../../data/task_0/GoToLocal/Task0_{level_name}_valid_{split_id}.pkl\", \"rb\") as f:\n",
    "        demos_valid = pkl.load(f)\n",
    "    \n",
    "    split_start_id = get_pretraining_data(\n",
    "        level_name, \n",
    "        demos_valid, \n",
    "        output_dir_path=\"../../data/task_0\",\n",
    "        is_train=False,\n",
    "        split_size=10000,\n",
    "        split_start_id=split_start_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7439a",
   "metadata": {},
   "source": [
    "# 2. Generate Task 0 BossLevel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e746b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blosc\n",
    "from collections import defaultdict\n",
    "import io\n",
    "import os\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5bae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretraining_data(\n",
    "    env,\n",
    "    data,\n",
    "    output_dir_path,\n",
    "    is_train,\n",
    "    split_size=100,\n",
    "    split_start_id=0,\n",
    "):\n",
    "    def image_to_bytes(image):\n",
    "        bytes_io = io.BytesIO()\n",
    "        image.save(bytes_io, \"PNG\")\n",
    "        return bytes_io.getvalue()\n",
    "    \n",
    "    output_dir = os.path.join(output_dir_path, env)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    split_id = 0\n",
    "    data_split = []\n",
    "    for ex_id, ex in tqdm(enumerate(data), total=len(data)):\n",
    "        img = blosc.unpack_array(ex[2])\n",
    "        img = Image.fromarray(img[0])  # Always select the first frame\n",
    "        img_encoded = image_to_bytes(img)\n",
    "        #data_split[\"img\"].append(img[0])  # Always select the first frame\n",
    "        \n",
    "        #data_split[\"mission\"].append(ex[0])\n",
    "\n",
    "        action = \",\".join(ex[6])\n",
    "        #data_split[\"action_seq\"].append(action)\n",
    "        data_split.append({\n",
    "            \"img\": img_encoded,\n",
    "            \"mission\": ex[0],\n",
    "            \"label\": ex[-1]\n",
    "        })\n",
    "        \n",
    "        if (ex_id+1) % split_size == 0:\n",
    "            if is_train:\n",
    "                output_name = f\"split_train_{split_start_id + split_id}\"\n",
    "            else:\n",
    "                output_name = f\"split_valid_{split_start_id + split_id}\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"{output_name}.pkl\")\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                pkl.dump(data_split, f)\n",
    "            data_split = []\n",
    "            split_id += 1\n",
    "\n",
    "    if data_split:\n",
    "        if is_train:\n",
    "            output_name = f\"split_train_{split_start_id + split_id}\"\n",
    "        else:\n",
    "            output_name = f\"split_valid_{split_start_id + split_id}\"\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{output_name}.pkl\")\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pkl.dump(data_split, f)\n",
    "    \n",
    "    return split_start_id + split_id  # used to track next starting splid_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4276b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positives and negatives for training split\n",
    "for split_id in range(10):\n",
    "    negatives, positives = [], []\n",
    "    with open(f\"../../data/task_0/BossLevel/negative_BabyAI-BossLevel-v0-split-{split_id}.pkl\", \"rb\") as f:\n",
    "        curr_negatives = pkl.load(f)\n",
    "        for i in range(len(curr_negatives)):\n",
    "            curr_negatives[i] = curr_negatives[i] + (\"No\",)\n",
    "    negatives += curr_negatives\n",
    "\n",
    "    with open(f\"../../data/task_0/BossLevel/positive_BabyAI-BossLevel-v0-split-{split_id}.pkl\", \"rb\") as f:\n",
    "        curr_positives = pkl.load(f)\n",
    "        for i in range(len(curr_positives)):\n",
    "            curr_positives[i] = curr_positives[i] + (\"Yes\",)\n",
    "    positives += curr_positives\n",
    "\n",
    "    data = positives + negatives\n",
    "\n",
    "    with open(f\"../../data/task_0/BossLevel/Task0_BossLevel_train_{split_id}.pkl\", \"wb\") as f:\n",
    "        pkl.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c134d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positives and negatives for validation split\n",
    "for split_id in range(10):\n",
    "    negatives, positives = [], []\n",
    "    with open(f\"../../data/task_0/BossLevel/negative_BabyAI-BossLevel-v0-split-{split_id}_valid.pkl\", \"rb\") as f:\n",
    "        curr_negatives = pkl.load(f)\n",
    "        for i in range(len(curr_negatives)):\n",
    "            curr_negatives[i] = curr_negatives[i] + (\"No\",)\n",
    "    negatives += curr_negatives\n",
    "\n",
    "    with open(f\"../../data/task_0/BossLevel/positive_BabyAI-BossLevel-v0-split-{split_id}_valid.pkl\", \"rb\") as f:\n",
    "        curr_positives = pkl.load(f)\n",
    "        for i in range(len(curr_positives)):\n",
    "            curr_positives[i] = curr_positives[i] + (\"Yes\",)\n",
    "    positives += curr_positives\n",
    "\n",
    "    data = positives + negatives\n",
    "\n",
    "    with open(f\"../../data/task_0/BossLevel/Task0_BossLevel_valid_{split_id}.pkl\", \"wb\") as f:\n",
    "        pkl.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f272780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:49<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:46<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:41<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:47<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:42<00:00, 22.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:45<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:38<00:00, 22.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:45<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:49<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:46<00:00, 22.10it/s]\n"
     ]
    }
   ],
   "source": [
    "level_name = \"BossLevel\"\n",
    "\n",
    "split_start_id = 0\n",
    "\n",
    "for split_id in range(10):\n",
    "    print(split_id)\n",
    "    with open(f\"../../data/task_0/{level_name}/Task0_{level_name}_train_{split_id}.pkl\", \"rb\") as f:\n",
    "        demos_train = pkl.load(f)\n",
    "    \n",
    "    split_start_id = get_pretraining_data(\n",
    "        level_name, \n",
    "        demos_train, \n",
    "        output_dir_path=\"../../data/task_0\",\n",
    "        is_train=True,\n",
    "        split_size=5000,\n",
    "        split_start_id=split_start_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9fe6f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:43<00:00, 23.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:51<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:45<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:45<00:00, 22.04it/s]\n"
     ]
    }
   ],
   "source": [
    "level_name = \"BossLevel\"\n",
    "\n",
    "split_start_id = 0\n",
    "\n",
    "for split_id in range(10):\n",
    "    print(split_id)\n",
    "    with open(f\"../../data/task_0/{level_name}/Task0_{level_name}_valid_{split_id}.pkl\", \"rb\") as f:\n",
    "        demos_valid = pkl.load(f)\n",
    "    \n",
    "    split_start_id = get_pretraining_data(\n",
    "        level_name, \n",
    "        demos_valid, \n",
    "        output_dir_path=\"../../data/task_0\",\n",
    "        is_train=False,\n",
    "        split_size=1000,\n",
    "        split_start_id=split_start_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445c7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data/task_0/BossLevel/split_train_0.pkl\", \"rb\") as f:\n",
    "    demos = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e2ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babyai_test.py\t       peek_at_data.ipynb  task_2.ipynb\r\n",
      "data_generation.ipynb  task_0.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db363047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.png\", \"wb\") as f:\n",
    "    f.write(demos[0][\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0789d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyai",
   "language": "python",
   "name": "babyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
